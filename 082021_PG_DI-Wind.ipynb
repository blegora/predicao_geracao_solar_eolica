{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import asizeof\n",
    "import metrics\n",
    "import kfold\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "numRuns = 10\n",
    "\n",
    "\n",
    "def getRollingWindow(index):\n",
    "    pivot = index\n",
    "    train_start = pivot.strftime('%Y-%m-%d')\n",
    "    pivot = pivot + datetime.timedelta(days=20)\n",
    "    train_end = pivot.strftime('%Y-%m-%d')\n",
    "\n",
    "    pivot = pivot + datetime.timedelta(days=1)\n",
    "    validation_start = pivot.strftime('%Y-%m-%d')\n",
    "    pivot = pivot + datetime.timedelta(days=6)\n",
    "    validation_end = pivot.strftime('%Y-%m-%d')\n",
    "\n",
    "    pivot = pivot + datetime.timedelta(days=1)\n",
    "    test_start = pivot.strftime('%Y-%m-%d')\n",
    "    pivot = pivot + datetime.timedelta(days=6)\n",
    "    test_end = pivot.strftime('%Y-%m-%d')\n",
    "    \n",
    "    return train_start, train_end, validation_start, validation_end, test_start, test_end\n",
    "\n",
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "\n",
    "\n",
    "seed = 3025731418\n",
    "seeds = np.random.mtrand.RandomState(seed)\n",
    "seeds = seeds.randint(0,4294967296,size=numRuns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df):\n",
    "    mindf = df.min()\n",
    "    maxdf = df.max()\n",
    "    return (df-mindf)/(maxdf-mindf)\n",
    "\n",
    "df = pd.read_csv('train.csv',  parse_dates=['date'], index_col=0)\n",
    "df.index = pd.to_datetime(df.index, format=\"%Y%m%d%H\")\n",
    "\n",
    "# Split data\n",
    "interval = ((df.index >= '2009-07') & (df.index <= '2010-07'))\n",
    "df = df.loc[interval]\n",
    "\n",
    "\n",
    "limit = df.index[-1].strftime('%Y-%m-%d')\n",
    "\n",
    "test_end = \"\"\n",
    "index = df.index[0]\n",
    "batches = []\n",
    "batches_supervised = []\n",
    "_order = 2\n",
    "_step = 1\n",
    "nobs = _order * len(df.columns)\n",
    "output_index = -len(df.columns)*_step\n",
    "\n",
    "while test_end < limit:\n",
    "\n",
    "    #print(\"Index: \", index.strftime('%Y-%m-%d'))  \n",
    "\n",
    "    train_start, train_end, validation_start, validation_end, test_start, test_end = getRollingWindow(index)\n",
    "    index = index + datetime.timedelta(days=7)\n",
    "    \n",
    "    train = df[train_start : train_end]\n",
    "    validation = df[validation_start : validation_end]\n",
    "    test = df[test_start : test_end]\n",
    "    train = train.append(validation)\n",
    "    \n",
    "    if len(batches) == 0:\n",
    "        batches.append(train)\n",
    "        train_reshaped_df = series_to_supervised(train,n_in=_order,n_out=_step)\n",
    "        train_X, train_Y = train_reshaped_df.iloc[:, :nobs].values, train_reshaped_df.iloc[:, output_index:].values\n",
    "        batches_supervised.append((train_X,train_Y))\n",
    "    \n",
    "    batches.append(test)\n",
    "    test_reshaped_df = series_to_supervised(test,n_in=_order,n_out=_step)\n",
    "    test_X, test_Y = test_reshaped_df.iloc[:, :nobs].values, test_reshaped_df.iloc[:, output_index:].values\n",
    "    batches_supervised.append((test_X,test_Y))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método eMVFTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from spatiotemporal.models.clusteredmvfts.fts import evolvingclusterfts\n",
    "#from ..source.emvfts.fts import evolvingclusterfts\n",
    "from pyFTS.models.multivariate import granular\n",
    "from spatiotemporal.util import benchmarks\n",
    "\n",
    "from pyFTS.benchmarks import Measures\n",
    "from spatiotemporal.data import loader\n",
    "import importlib\n",
    "import copy\n",
    "import asizeof\n",
    "\n",
    "\n",
    "step = 1\n",
    "evolfts_order = 2\n",
    "tend_evolving = []\n",
    "mems_evolving = []\n",
    "#file_evolving = open('file_evolving.pickle','wb')\n",
    "#exp_name = \"BENCHMARK-2_1-EVOLVING\"\n",
    "forecasts = []\n",
    "\n",
    "_variance_limit = 0.001\n",
    "_defuzzy = 'weighted'\n",
    "_t_norm = 'threshold'\n",
    "_membership_threshold = 0.6\n",
    "_order = 2\n",
    "_step = 1\n",
    "\n",
    "tstart = time.time()\n",
    "model = evolvingclusterfts.EvolvingClusterFTS(variance_limit=_variance_limit, defuzzy=_defuzzy, t_norm=_t_norm,\n",
    "                                              membership_threshold=_membership_threshold)\n",
    "\n",
    "\n",
    "model.fit(batches[0].values, order=_order, verbose=False)\n",
    "\n",
    "\n",
    "forecasts = [[]]*(len(batches)-1)\n",
    "forecasts_emvfts_runs = []\n",
    "for j in range(1,len(batches)):\n",
    "    forecast = model.predict(batches[j].values,steps_ahead=_step)\n",
    "    forecast_df = pd.DataFrame(data=forecast, columns=batches[0].columns)\n",
    "    forecasts[j-1] = forecast_df.values\n",
    "    \n",
    "# Para economizar tempo (o método é determinístico, então sempre retornará as mesmas saídas)\n",
    "# O código executa uma vez e replica a lista pelo número de vezes que o código deseja rodar\n",
    "# Isso é feito para manter o padrão das outras técnicas, e assim o mesmo código que processa\n",
    "# os resultados pode ser usado para todas as técnicas. Não é uma gambiarra!!!\n",
    "\n",
    "tempo_emvfts_runs = [time.time() - tstart]*numRuns #Gambiarra\n",
    "memoria_emvfts_runs = [asizeof.asizeof(model)]*numRuns #Gambiarra\n",
    "forecasts_emvfts_runs = [forecasts]*numRuns #Gambiarra\n",
    "\n",
    "save_obj = (forecasts_emvfts_runs,tempo_emvfts_runs,memoria_emvfts_runs)\n",
    "\n",
    "with open('wind_emvfts.pickle','wb') as file:\n",
    "    pickle.dump(save_obj,file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mondrian Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mondrianforest import MondrianForestRegressor\n",
    "from mfr import MFR\n",
    "\n",
    "x,y = batches_supervised[0]\n",
    "numModels = y.shape[1]\n",
    "\n",
    "paramNames = ['n_trees']\n",
    "paramValues = [list(range(1,26))]\n",
    "\n",
    "mfrlambda = lambda x={}: MFR(x)\n",
    "paramStructs = [[]]*numModels\n",
    "\n",
    "for i in range(0,numModels):\n",
    "    kfoldcv = kfold.KFold(5, mfrlambda, paramNames,paramValues,metrics.RMSE())\n",
    "    paramStructs[i],_ = kfoldcv.start(x,y[:,i].reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A implementação do Mondrian Forest só aceita uma saída por vez\n",
    "\n",
    "forecasts_mondrian_runs = [[]]*numRuns\n",
    "memoria_mondrian_runs = [[]]*numRuns\n",
    "tempo_mondrian_runs = [[]]*numRuns\n",
    "\n",
    "for k in range(0,numRuns):\n",
    "    tstart = time.time()\n",
    "    x,y = batches_supervised[0]\n",
    "    numModels = y.shape[1]\n",
    "    models = [[]]*numModels\n",
    "    for i in range(0,numModels):\n",
    "        models[i] = MFR(dict({'seed':seeds[k]},**paramStructs[i]))\n",
    "        models[i].train(x,y[:,i])\n",
    "\n",
    "    forecasts = [[]]*(len(batches_supervised)-1)\n",
    "    for j in range(1,len(batches_supervised)):\n",
    "        x,y = batches_supervised[j]\n",
    "        forecast = [[]]*numModels\n",
    "        for i in range(0,numModels):\n",
    "            forecast[i] = models[i].predict(x).reshape((-1,1))\n",
    "        forecasts[j-1] = np.hstack(forecast) \n",
    "\n",
    "        for i in range(0,numModels):\n",
    "            models[i].train(x,y[:,i])\n",
    "    forecasts_mondrian_runs[k] = forecasts\n",
    "    tempo_mondrian_runs[k] = time.time() - tstart\n",
    "    memoria_mondrian_runs[k] = sum([asizeof.asizeof(x) for x in models])\n",
    "    \n",
    "    \n",
    "save_obj = (forecasts_mondrian_runs,tempo_mondrian_runs,memoria_mondrian_runs,paramStructs)\n",
    "\n",
    "with open('wind_mondrian.pickle','wb') as file:\n",
    "    pickle.dump(save_obj,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OSRELM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import osrelm\n",
    "\n",
    "x,y = batches_supervised[0]\n",
    "\n",
    "paramNames = ['regularization_parameter','number_of_hidden_neurons',]\n",
    "paramValues = [[2**x for x in range(-20,21)],[10,50,100,200,300,400,500,600,700,800,900,1000]]\n",
    "\n",
    "osrelmlambda = lambda d={}: osrelm.OSRELM(dict({'number_of_input_neurons':x.shape[1]},**d))\n",
    "\n",
    "kfoldcv = kfold.KFold(5, osrelmlambda, paramNames,paramValues,metrics.RMSE())\n",
    "paramStruct,_ = kfoldcv.start(x,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts_osrelm_runs = [[]]*numRuns\n",
    "memoria_osrelm_runs = [[]]*numRuns\n",
    "tempo_osrelm_runs = [[]]*numRuns\n",
    "\n",
    "for k in range(0,numRuns):\n",
    "    tstart = time.time()\n",
    "    x,y = batches_supervised[0]\n",
    "    \n",
    "    model = osrelm.OSRELM(dict({'number_of_input_neurons':x.shape[1],'seed':seeds[k]},**paramStruct))\n",
    "    model.train(x,y)\n",
    "\n",
    "    forecasts = [[]]*(len(batches_supervised)-1)\n",
    "    for j in range(1,len(batches_supervised)):\n",
    "        x,y = batches_supervised[j]\n",
    "        forecasts[j-1] = model.predict(x)\n",
    "        model.train(x,y)\n",
    "    tempo_osrelm_runs[k] = time.time() - tstart\n",
    "    forecasts_osrelm_runs[k] = forecasts\n",
    "    memoria_osrelm_runs[k] = asizeof.asizeof(model)\n",
    "    \n",
    "save_obj = (forecasts_osrelm_runs,tempo_osrelm_runs,memoria_osrelm_runs,paramStruct)\n",
    "\n",
    "with open('wind_osrelm.pickle','wb') as file:\n",
    "    pickle.dump(save_obj,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import SGDRegressor\n",
    "from sgdr import SGDR\n",
    "\n",
    "x,y = batches_supervised[0]\n",
    "numModels = y.shape[1]\n",
    "\n",
    "paramNames = ['loss','penalty','alpha','l1_ratio','learning_rate','eta0']\n",
    "paramValues = [['squared_loss','huber','epsilon_insensitive'],['l2','l1','elasticnet'],\\\n",
    "               [2**x for x in range(-20,21)],[0.15,0.5,0.75],['constant','optimal','invscaling','adaptive'],[0.01,0.05,0.1]]\n",
    "\n",
    "sgdrlambda = lambda x={}: SGDR(x)\n",
    "paramStructs = [[]]*numModels\n",
    "\n",
    "for i in range(0,numModels):\n",
    "    kfoldcv = kfold.KFold(5, sgdrlambda, paramNames,paramValues,metrics.RMSE())\n",
    "    paramStructs[i],_ = kfoldcv.start(x,y[:,i].reshape(-1,1))\n",
    "    #paramStructs[i] = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A implementação do método só aceita uma saída por vez\n",
    "forecasts_sgdr_runs = [[]]*numRuns\n",
    "memoria_sgdr_runs = [[]]*numRuns\n",
    "tempo_sgdr_runs = [[]]*numRuns\n",
    "for k in range(0,numRuns):\n",
    "    tstart = time.time()\n",
    "    x,y = batches_supervised[0]\n",
    "    numModels = y.shape[1]\n",
    "    models = [[]]*numModels\n",
    "    for i in range(0,numModels):\n",
    "        models[i] = SGDR(dict({'seed':seeds[k]},**paramStructs[i]))\n",
    "        models[i].train(x,y[:,i])\n",
    "\n",
    "    forecasts = [[]]*(len(batches_supervised)-1)\n",
    "    for j in range(1,len(batches_supervised)):\n",
    "        x,y = batches_supervised[j]\n",
    "        forecast = [[]]*numModels\n",
    "        for i in range(0,numModels):\n",
    "            forecast[i] = models[i].predict(x).reshape((-1,1))\n",
    "        forecasts[j-1] = np.hstack(forecast) \n",
    "\n",
    "        for i in range(0,numModels):\n",
    "            models[i].train(x,y[:,i])\n",
    "    tempo_sgdr_runs[k] = time.time() - tstart\n",
    "    memoria_sgdr_runs[k] = sum([asizeof.asizeof(x) for x in models])\n",
    "    forecasts_sgdr_runs[k] = forecasts\n",
    "    \n",
    "save_obj = (forecasts_sgdr_runs,tempo_sgdr_runs,memoria_sgdr_runs,paramStructs)\n",
    "\n",
    "with open('wind_sgdr.pickle','wb') as file:\n",
    "    pickle.dump(save_obj,file)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Passive Agressive Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import PassiveAggressiveRegressor\n",
    "from par import PAR\n",
    "\n",
    "# C - regularização - 1.0\n",
    "# epsilon - threshold para atualizar - 0.1\n",
    "# fit_intercept = True\n",
    "# n_iter = epocas. 5\n",
    "# loss = 'epsilon_insensitive' / 'squared_epsilon_insensitive'\n",
    "\n",
    "x,y = batches_supervised[0]\n",
    "numModels = y.shape[1]\n",
    "\n",
    "paramNames = ['C','loss']\n",
    "paramValues = [[2**x for x in range(-20,21)],['epsilon_insensitive','squared_epsilon_insensitive']]\n",
    "\n",
    "parlambda = lambda x={}: PAR(x)\n",
    "paramStructs = [[]]*numModels\n",
    "\n",
    "for i in range(0,numModels):\n",
    "    kfoldcv = kfold.KFold(5, parlambda, paramNames,paramValues,metrics.RMSE())\n",
    "    paramStructs[i],_ = kfoldcv.start(x,y[:,i].reshape(-1,1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A implementação do método só aceita uma saída por vez\n",
    "forecasts_par_runs = [[]]*numRuns\n",
    "tempo_par_runs = [[]]*numRuns\n",
    "memoria_par_runs = [[]]*numRuns\n",
    "\n",
    "for k in range(0,numRuns):\n",
    "    tstart = time.time()\n",
    "    x,y = batches_supervised[0]\n",
    "    numModels = y.shape[1]\n",
    "    models = [[]]*numModels\n",
    "    for i in range(0,numModels):\n",
    "        models[i] = PAR(dict({'seed':seeds[k]},**paramStructs[i]))\n",
    "        models[i].train(x,y[:,i])\n",
    "\n",
    "    forecasts = [[]]*(len(batches_supervised)-1)\n",
    "    for j in range(1,len(batches_supervised)):\n",
    "        x,y = batches_supervised[j]\n",
    "        forecast = [[]]*numModels\n",
    "        for i in range(0,numModels):\n",
    "            forecast[i] = models[i].predict(x).reshape((-1,1))\n",
    "        forecasts[j-1] = np.hstack(forecast) \n",
    "\n",
    "        for i in range(0,numModels):\n",
    "            models[i].train(x,y[:,i])\n",
    "    tempo_par_runs[k] = time.time() - tstart\n",
    "    forecasts_par_runs[k] = forecasts\n",
    "    memoria_par_runs[k] = sum([asizeof.asizeof(x) for x in models])\n",
    "    \n",
    "save_obj = (forecasts_par_runs,tempo_par_runs,memoria_par_runs,paramStructs)\n",
    "\n",
    "with open('wind_par.pickle','wb') as file:\n",
    "    pickle.dump(save_obj,file)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tet",
   "language": "python",
   "name": "tet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "390.667px",
    "left": "708px",
    "right": "20px",
    "top": "149px",
    "width": "514px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
