{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import asizeof\n",
    "import metrics\n",
    "import kfold\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "numRuns = 1\n",
    "\n",
    "\n",
    "def getRollingWindow(index):\n",
    "    pivot = index\n",
    "    train_start = pivot.strftime('%Y-%m-%d')\n",
    "    pivot = pivot + datetime.timedelta(days=20)\n",
    "    train_end = pivot.strftime('%Y-%m-%d')\n",
    "\n",
    "    pivot = pivot + datetime.timedelta(days=1)\n",
    "    validation_start = pivot.strftime('%Y-%m-%d')\n",
    "    pivot = pivot + datetime.timedelta(days=6)\n",
    "    validation_end = pivot.strftime('%Y-%m-%d')\n",
    "\n",
    "    pivot = pivot + datetime.timedelta(days=1)\n",
    "    test_start = pivot.strftime('%Y-%m-%d')\n",
    "    pivot = pivot + datetime.timedelta(days=6)\n",
    "    test_end = pivot.strftime('%Y-%m-%d')\n",
    "    \n",
    "    return train_start, train_end, validation_start, validation_end, test_start, test_end\n",
    "\n",
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "\n",
    "\n",
    "seed = 3025731418\n",
    "seeds = np.random.mtrand.RandomState(seed)\n",
    "seeds = seeds.randint(0,4294967296,size=numRuns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/bruno/Dropbox/PG INF/EMVFTS/notebooks/clean', '/usr/lib/python38.zip', '/usr/lib/python3.8', '/usr/lib/python3.8/lib-dynload', '', '/home/bruno/tet/lib/python3.8/site-packages', '/home/bruno/tet/lib/python3.8/site-packages/IPython/extensions', '/home/bruno/.ipython']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df):\n",
    "    mindf = df.min()\n",
    "    maxdf = df.max()\n",
    "    return (df-mindf)/(maxdf-mindf)\n",
    "\n",
    "#df = pd.read_csv('https://query.data.world/s/wo5wryokqyg5uvbfqqij2mucgwly5u',  parse_dates=['datetime'], index_col=0)\n",
    "df = pd.read_csv('solar_oahu_df.csv',  parse_dates=['datetime'], index_col=0)\n",
    "\n",
    "df = normalize(df)\n",
    "\n",
    "# Split data\n",
    "interval = ((df.index >= '2010-06') & (df.index < '2011-06'))\n",
    "df = df.loc[interval]\n",
    "\n",
    "\n",
    "\n",
    "limit = df.index[-1].strftime('%Y-%m-%d')\n",
    "\n",
    "test_end = \"\"\n",
    "index = df.index[0]\n",
    "batches = []\n",
    "batches_supervised = []\n",
    "_order = 2\n",
    "_step = 1\n",
    "nobs = _order * len(df.columns)\n",
    "output_index = -len(df.columns)*_step\n",
    "\n",
    "while test_end < limit:\n",
    "\n",
    "    #print(\"Index: \", index.strftime('%Y-%m-%d'))  \n",
    "\n",
    "    train_start, train_end, validation_start, validation_end, test_start, test_end = getRollingWindow(index)\n",
    "    index = index + datetime.timedelta(days=7)\n",
    "    \n",
    "    train = df[train_start : train_end]\n",
    "    validation = df[validation_start : validation_end]\n",
    "    test = df[test_start : test_end]\n",
    "    train = train.append(validation)\n",
    "    \n",
    "    if len(batches) == 0:\n",
    "        batches.append(train)\n",
    "        train_reshaped_df = series_to_supervised(train,n_in=_order,n_out=_step)\n",
    "        train_X, train_Y = train_reshaped_df.iloc[:, :nobs].values, train_reshaped_df.iloc[:, output_index:].values\n",
    "        batches_supervised.append((train_X,train_Y))\n",
    "    \n",
    "    batches.append(test)\n",
    "    test_reshaped_df = series_to_supervised(test,n_in=_order,n_out=_step)\n",
    "    test_X, test_Y = test_reshaped_df.iloc[:, :nobs].values, test_reshaped_df.iloc[:, output_index:].values\n",
    "    batches_supervised.append((test_X,test_Y))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MÃ©todo eMVFTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8835/629646932.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mspatiotemporal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclusteredmvfts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfts\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mevolvingclusterfts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#from ..source.emvfts.fts import evolvingclusterfts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyFTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultivariate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgranular\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspatiotemporal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbenchmarks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tet/lib/python3.8/site-packages/spatiotemporal/models/clusteredmvfts/fts/evolvingclusterfts.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyFTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflrg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitioner\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEvolvingClusteringPartitioner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tet/lib/python3.8/site-packages/pyFTS/common/fts.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyFTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFuzzySet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSortedCollection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUtil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tet/lib/python3.8/site-packages/pyFTS/common/Util.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcm\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcmx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolors\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpltcolors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyFTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobabilistic\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mProbabilityDistribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyFTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTransformations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tet/lib/python3.8/site-packages/pyFTS/probabilistic/ProbabilityDistribution.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyFTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFuzzySet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSortedCollection\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyFTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobabilistic\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkde\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tet/lib/python3.8/site-packages/pyFTS/probabilistic/kde.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \"\"\"\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyFTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTransformations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tet/lib/python3.8/site-packages/pyFTS/common/Transformations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyFTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearTrend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyFTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSOMTransformation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyFTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautoencoder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoencoderTransformation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyFTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNormalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tet/lib/python3.8/site-packages/pyFTS/common/transformations/autoencoder.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregularizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tet/lib/python3.8/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \"\"\"\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from spatiotemporal.models.clusteredmvfts.fts import evolvingclusterfts\n",
    "#from ..source.emvfts.fts import evolvingclusterfts\n",
    "from pyFTS.models.multivariate import granular\n",
    "from spatiotemporal.util import benchmarks\n",
    "\n",
    "from pyFTS.benchmarks import Measures\n",
    "from spatiotemporal.data import loader\n",
    "import importlib\n",
    "import copy\n",
    "import asizeof\n",
    "\n",
    "\n",
    "step = 1\n",
    "evolfts_order = 2\n",
    "tend_evolving = []\n",
    "mems_evolving = []\n",
    "#file_evolving = open('file_evolving.pickle','wb')\n",
    "#exp_name = \"BENCHMARK-2_1-EVOLVING\"\n",
    "forecasts = []\n",
    "\n",
    "_variance_limit = 0.001\n",
    "_defuzzy = 'weighted'\n",
    "_t_norm = 'threshold'\n",
    "_membership_threshold = 0.6\n",
    "_order = 2\n",
    "_step = 1\n",
    "\n",
    "tstart = time.time()\n",
    "model = evolvingclusterfts.EvolvingClusterFTS(variance_limit=_variance_limit, defuzzy=_defuzzy, t_norm=_t_norm,\n",
    "                                              membership_threshold=_membership_threshold)\n",
    "\n",
    "\n",
    "model.fit(batches[0].values, order=_order, verbose=False)\n",
    "\n",
    "m1 = []\n",
    "t1 = [time.time() - tstart]\n",
    "\n",
    "forecasts = [[]]*(len(batches)-1)\n",
    "forecasts_emvfts_runs = []\n",
    "for j in range(1,len(batches)):\n",
    "    auxTime = time.time()\n",
    "    forecast = model.predict(batches[j].values,steps_ahead=_step)\n",
    "    t1.append(time.time()-auxTime)\n",
    "    m1.append(asizeof.asizeof(model))\n",
    "    forecast_df = pd.DataFrame(data=forecast, columns=batches[0].columns)\n",
    "    forecasts[j-1] = forecast_df.values\n",
    "    \n",
    "# Para economizar tempo (o mÃ©todo Ã© determinÃ­stico, entÃ£o sempre retornarÃ¡ as mesmas saÃ­das)\n",
    "# O cÃ³digo executa uma vez e replica a lista pelo nÃºmero de vezes que o cÃ³digo deseja rodar\n",
    "# Isso Ã© feito para manter o padrÃ£o das outras tÃ©cnicas, e assim o mesmo cÃ³digo que processa\n",
    "# os resultados pode ser usado para todas as tÃ©cnicas. NÃ£o Ã© uma gambiarra!!!\n",
    "\n",
    "tempo_emvfts_runs = [time.time() - tstart]*numRuns #Gambiarra\n",
    "memoria_emvfts_runs = [asizeof.asizeof(model)]*numRuns #Gambiarra\n",
    "forecasts_emvfts_runs = [forecasts]*numRuns #Gambiarra\n",
    "\n",
    "save_obj = (forecasts_emvfts_runs,tempo_emvfts_runs,memoria_emvfts_runs)\n",
    "# save_obj = (m1,t1)\n",
    "\n",
    "with open('solar_emvfts_m1t1.pickle','wb') as file:\n",
    "    pickle.dump(save_obj,file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(m1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mondrian Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mondrianforest import MondrianForestRegressor\n",
    "from mfr import MFR\n",
    "\n",
    "x,y = batches_supervised[0]\n",
    "numModels = y.shape[1]\n",
    "\n",
    "paramNames = ['n_trees']\n",
    "paramValues = [list(range(1,26))]\n",
    "\n",
    "mfrlambda = lambda x={}: MFR(x)\n",
    "paramStructs = [[]]*numModels\n",
    "\n",
    "# for i in range(0,numModels):\n",
    "#     kfoldcv = kfold.KFold(5, mfrlambda, paramNames,paramValues,metrics.RMSE())\n",
    "#     paramStructs[i],_ = kfoldcv.start(x,y[:,i].reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(_,_,_,paramStructs) = pickle.load(open('solar_mondrian.pickle','rb'))\n",
    "from mfr import MFR\n",
    "\n",
    "\n",
    "#A implementaÃ§Ã£o do Mondrian Forest sÃ³ aceita uma saÃ­da por vez\n",
    "\n",
    "forecasts_mondrian_runs = [[]]*numRuns\n",
    "memoria_mondrian_runs = [[]]*numRuns\n",
    "tempo_mondrian_runs = [[]]*numRuns\n",
    "\n",
    "t1 = []\n",
    "m1 = []\n",
    "\n",
    "for k in range(0,numRuns):\n",
    "    tstart = time.time()\n",
    "    x,y = batches_supervised[0]\n",
    "    numModels = y.shape[1]\n",
    "    models = [[]]*numModels\n",
    "    for i in range(0,numModels):\n",
    "        models[i] = MFR(dict({'seed':seeds[k]},**paramStructs[i]))\n",
    "        models[i].train(x,y[:,i])\n",
    "    t1 = [time.time() - tstart]\n",
    "    m1.append(sum([asizeof.asizeof(x) for x in models]))\n",
    "    \n",
    "    forecasts = [[]]*(len(batches_supervised)-1)\n",
    "    for j in range(1,len(batches_supervised)):\n",
    "        x,y = batches_supervised[j]\n",
    "        forecast = [[]]*numModels\n",
    "        for i in range(0,numModels):\n",
    "            forecast[i] = models[i].predict(x).reshape((-1,1))\n",
    "        forecasts[j-1] = np.hstack(forecast) \n",
    "        \n",
    "        auxTime = time.time()\n",
    "        for i in range(0,numModels):\n",
    "            models[i].train(x,y[:,i])\n",
    "        t1.append(time.time() - auxTime)\n",
    "        m1.append(sum([asizeof.asizeof(x) for x in models]))\n",
    "    forecasts_mondrian_runs[k] = forecasts\n",
    "    tempo_mondrian_runs[k] = time.time() - tstart\n",
    "    memoria_mondrian_runs[k] = sum([asizeof.asizeof(x) for x in models])\n",
    "    \n",
    "    \n",
    "save_obj = (forecasts_mondrian_runs,tempo_mondrian_runs,memoria_mondrian_runs,paramStructs)\n",
    "save_obj = (m1,t1)\n",
    "\n",
    "with open('solar_mondrian_m1t1.pickle','wb') as file:\n",
    "    pickle.dump(save_obj,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(m1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OSRELM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import osrelm\n",
    "\n",
    "x,y = batches_supervised[0]\n",
    "\n",
    "paramNames = ['regularization_parameter','number_of_hidden_neurons',]\n",
    "paramValues = [[2**x for x in range(-20,21)],[10,50,100,200,300,400,500,600,700,800,900,1000]]\n",
    "\n",
    "osrelmlambda = lambda d={}: osrelm.OSRELM(dict({'number_of_input_neurons':x.shape[1]},**d))\n",
    "\n",
    "kfoldcv = kfold.KFold(5, osrelmlambda, paramNames,paramValues,metrics.RMSE())\n",
    "# paramStruct,_ = kfoldcv.start(x,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import osrelm\n",
    "(_,_,_,paramStruct) = pickle.load(open('solar_osrelm.pickle','rb'))\n",
    "\n",
    "forecasts_osrelm_runs = [[]]*numRuns\n",
    "memoria_osrelm_runs = [[]]*numRuns\n",
    "tempo_osrelm_runs = [[]]*numRuns\n",
    "\n",
    "m1=[]\n",
    "t1 = []\n",
    "\n",
    "for k in range(0,numRuns):\n",
    "    tstart = time.time()\n",
    "    x,y = batches_supervised[0]\n",
    "    \n",
    "    model = osrelm.OSRELM(dict({'number_of_input_neurons':x.shape[1],'seed':seeds[k]},**paramStruct))\n",
    "    model.train(x,y)\n",
    "    t1 = [time.time() - tstart]\n",
    "\n",
    "    forecasts = [[]]*(len(batches_supervised)-1)\n",
    "    for j in range(1,len(batches_supervised)):\n",
    "        x,y = batches_supervised[j]\n",
    "        forecasts[j-1] = model.predict(x)\n",
    "        auxTime = time.time()\n",
    "        model.train(x,y)\n",
    "        t1.append(time.time() - auxTime)\n",
    "        m1.append(asizeof.asizeof(model))\n",
    "    tempo_osrelm_runs[k] = time.time() - tstart\n",
    "    forecasts_osrelm_runs[k] = forecasts\n",
    "    memoria_osrelm_runs[k] = asizeof.asizeof(model)\n",
    "    \n",
    "save_obj = (forecasts_osrelm_runs,tempo_osrelm_runs,memoria_osrelm_runs,paramStruct)\n",
    "save_obj = (m1,t1)\n",
    "\n",
    "with open('solar_osrelm_m1t1.pickle','wb') as file:\n",
    "    pickle.dump(save_obj,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(m1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import SGDRegressor\n",
    "from sgdr import SGDR\n",
    "\n",
    "x,y = batches_supervised[0]\n",
    "numModels = y.shape[1]\n",
    "\n",
    "paramNames = ['loss','penalty','alpha','l1_ratio','learning_rate','eta0']\n",
    "paramValues = [['squared_loss','huber','epsilon_insensitive'],['l2','l1','elasticnet'],\\\n",
    "               [2**x for x in range(-20,21)],[0.15,0.5,0.75],['constant','optimal','invscaling','adaptive'],[0.01,0.05,0.1]]\n",
    "\n",
    "sgdrlambda = lambda x={}: SGDR(x)\n",
    "paramStructs = [[]]*numModels\n",
    "\n",
    "for i in range(0,numModels):\n",
    "    kfoldcv = kfold.KFold(5, sgdrlambda, paramNames,paramValues,metrics.RMSE())\n",
    "    #paramStructs[i],_ = kfoldcv.start(x,y[:,i].reshape(-1,1))\n",
    "    paramStructs[i] = {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(_,_,_,paramStructs) = pickle.load(open('solar_sgdr.pickle','rb'))\n",
    "\n",
    "# A implementaÃ§Ã£o do mÃ©todo sÃ³ aceita uma saÃ­da por vez\n",
    "forecasts_sgdr_runs = [[]]*numRuns\n",
    "memoria_sgdr_runs = [[]]*numRuns\n",
    "tempo_sgdr_runs = [[]]*numRuns\n",
    "\n",
    "m1 = []\n",
    "t1 = []\n",
    "for k in range(0,numRuns):\n",
    "    tstart = time.time()\n",
    "    x,y = batches_supervised[0]\n",
    "    numModels = y.shape[1]\n",
    "    models = [[]]*numModels\n",
    "    for i in range(0,numModels):\n",
    "        models[i] = SGDR(dict({'seed':seeds[k]},**paramStructs[i]))\n",
    "        models[i].train(x,y[:,i])\n",
    "    t1.append(time.time() - tstart)\n",
    "\n",
    "    forecasts = [[]]*(len(batches_supervised)-1)\n",
    "    for j in range(1,len(batches_supervised)):\n",
    "        x,y = batches_supervised[j]\n",
    "        forecast = [[]]*numModels\n",
    "        for i in range(0,numModels):\n",
    "            forecast[i] = models[i].predict(x).reshape((-1,1))\n",
    "        forecasts[j-1] = np.hstack(forecast) \n",
    "        auxTime = time.time()\n",
    "        for i in range(0,numModels):\n",
    "            models[i].train(x,y[:,i])\n",
    "        t1.append(time.time() - auxTime)\n",
    "        m1.append(sum([asizeof.asizeof(x) for x in models]))\n",
    "    tempo_sgdr_runs[k] = time.time() - tstart\n",
    "    memoria_sgdr_runs[k] = sum([asizeof.asizeof(x) for x in models])\n",
    "    forecasts_sgdr_runs[k] = forecasts\n",
    "    \n",
    "save_obj = (forecasts_sgdr_runs,tempo_sgdr_runs,memoria_sgdr_runs,paramStructs)\n",
    "save_obj = (m1,t1)\n",
    "\n",
    "with open('solar_sgdr_m1t1.pickle','wb') as file:\n",
    "    pickle.dump(save_obj,file)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(m1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Passive Agressive Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import PassiveAggressiveRegressor\n",
    "from par import PAR\n",
    "\n",
    "# C - regularizaÃ§Ã£o - 1.0\n",
    "# epsilon - threshold para atualizar - 0.1\n",
    "# fit_intercept = True\n",
    "# n_iter = epocas. 5\n",
    "# loss = 'epsilon_insensitive' / 'squared_epsilon_insensitive'\n",
    "\n",
    "x,y = batches_supervised[0]\n",
    "numModels = y.shape[1]\n",
    "\n",
    "paramNames = ['C','loss']\n",
    "paramValues = [[2**x for x in range(-20,21)],['epsilon_insensitive','squared_epsilon_insensitive']]\n",
    "\n",
    "parlambda = lambda x={}: PAR(x)\n",
    "paramStructs = [[]]*numModels\n",
    "\n",
    "# for i in range(0,numModels):\n",
    "#     kfoldcv = kfold.KFold(5, parlambda, paramNames,paramValues,metrics.RMSE())\n",
    "#     paramStructs[i],_ = kfoldcv.start(x,y[:,i].reshape(-1,1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(_,_,_,paramStructs) = pickle.load(open('solar_par.pickle','rb'))\n",
    "from par import PAR\n",
    "\n",
    "# A implementaÃ§Ã£o do mÃ©todo sÃ³ aceita uma saÃ­da por vez\n",
    "forecasts_par_runs = [[]]*numRuns\n",
    "tempo_par_runs = [[]]*numRuns\n",
    "memoria_par_runs = [[]]*numRuns\n",
    "\n",
    "m1 = []\n",
    "t1 = []\n",
    "\n",
    "for k in range(0,numRuns):\n",
    "    tstart = time.time() \n",
    "    x,y = batches_supervised[0]\n",
    "    numModels = y.shape[1]\n",
    "    models = [[]]*numModels\n",
    "    for i in range(0,numModels):\n",
    "        models[i] = PAR(dict({'seed':seeds[k]},**paramStructs[i]))\n",
    "        models[i].train(x,y[:,i])\n",
    "    t1.append(time.time() - tstart)\n",
    "    \n",
    "    forecasts = [[]]*(len(batches_supervised)-1)\n",
    "    for j in range(1,len(batches_supervised)):\n",
    "        x,y = batches_supervised[j]\n",
    "        forecast = [[]]*numModels\n",
    "        for i in range(0,numModels):\n",
    "            forecast[i] = models[i].predict(x).reshape((-1,1))\n",
    "        forecasts[j-1] = np.hstack(forecast) \n",
    "        auxTime = time.time()\n",
    "        \n",
    "        for i in range(0,numModels):\n",
    "            models[i].train(x,y[:,i])\n",
    "        t1.append(time.time() - auxTime)\n",
    "        m1.append(sum([asizeof.asizeof(x) for x in models]))\n",
    "    tempo_par_runs[k] = time.time() - tstart\n",
    "    forecasts_par_runs[k] = forecasts\n",
    "    memoria_par_runs[k] = sum([asizeof.asizeof(x) for x in models])\n",
    "    \n",
    "save_obj = (forecasts_par_runs,tempo_par_runs,memoria_par_runs,paramStructs)\n",
    "save_obj = (m1,t1)\n",
    "\n",
    "with open('solar_par_m1t1.pickle','wb') as file:\n",
    "    pickle.dump(save_obj,file)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(m1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t1)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tet",
   "language": "python",
   "name": "tet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "390.667px",
    "left": "708px",
    "right": "20px",
    "top": "152px",
    "width": "514px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
